# -*- coding: utf-8 -*-
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

import pandas as pd
import pytz
from google.cloud import bigquery
from prefect import task
from selenium import webdriver
from selenium.common.exceptions import WebDriverException
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

from pipelines.constants import constants
from pipelines.datalake.extract_load.diario_oficial_uniao.utils import (
    extract_decree_details,
)
from pipelines.utils.credential_injector import authenticated_task as task
from pipelines.utils.logger import log
from pipelines.utils.tasks import upload_df_to_datalake
from pipelines.utils.time import parse_date_or_today

# Configura√ß√µes do Web Driver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--disable-plugins")
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-images")
chrome_options.add_argument("--page-load-strategy=eager")


@task(nout=2)
def dou_extraction(dou_section: int, max_workers: int, date: datetime) -> list:
    """Tarefa para extra√ß√£o dos dados de atos oficiais do DOU.

    Args:
        dou_section (int): Se√ß√£o do DOU a ser extra√≠da.
        max_workers (int): N√∫mero m√°ximo de workers para execu√ß√£o paralela.
        date (datetime): Data do di√°rio oficial a ser extra√≠do.

    Raises:
        exc: M√°ximo de tentativas alcan√ßado na requisi√ß√£o de algum ato oficial.

    Returns:
        list: Lista de dicion√°rios contendo os dados extra√≠dos de cada ato do DOU e a vari√°vel que indica que a extra√ß√£o foi bem sucedida.

    """
    date = parse_date_or_today(date)

    items = []  # Lista de dicion√°rios com os metadados e informa√ß√µes de cada ato
    page_count = 1

    day, month, year = date.day, date.month, date.year

    driver = webdriver.Chrome(options=chrome_options)
    log("ü§ñ Iniciando o webdriver...")
    log(
        f"Iniciando extra√ß√£o dos atos oficiais do DOU Se√ß√£o {str(dou_section)} de {date.strftime('%d/%m/%Y')}"
    )

    try:
        driver.get(
            f"https://www.in.gov.br/leiturajornal?data={day}-{month}-{year}"
            f"&secao=do{str(dou_section)}"
        )
    except WebDriverException:
        log("‚ùå Erro ao acessar o site do DOU.")
        return [[], False]

    while True:
        log(f"Extra√ß√£o da p√°gina {page_count}")

        # Quando n√£o h√° atos oficias, h√° um elemento de aviso
        if driver.find_elements(by=By.CLASS_NAME, value="alert.alert-info"):
            log("‚ö†Ô∏è N√£o h√° atos oficias para extrair.")
            break

        cards = driver.find_elements(by=By.CLASS_NAME, value="resultado")

        decree_links_to_process = []  # Lista para armazenar os links dos decretos

        # Pegando a url de cada ato na p√°gina de pesquisa
        for card in cards:
            text_link = card.find_element(by=By.TAG_NAME, value="a")
            decree_links_to_process.append(text_link.get_attribute("href"))

        # Implementando extracao com concorr√™ncia
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(extract_decree_details, url): url for url in decree_links_to_process
            }

            for i, future in enumerate(as_completed(future_to_url)):
                url = future_to_url[future]
                try:
                    decree_details = future.result()
                    items.append(decree_details)
                except Exception as exc:
                    log(
                        f"‚ùå A requisi√ß√£o para {url} alcan√ßou o m√°ximo de tentativas na requisi√ß√£o."
                    )
                    log(f"‚ùå Erro: {exc}")
                    return [[], False]
        # Buscando o bot√£o para a pr√≥xima p√°gina de pesquisa
        pagination_buttons = driver.find_elements(by=By.CLASS_NAME, value="pagination-button")
        next_btn = None

        if pagination_buttons:
            for button in pagination_buttons:
                if "Pr√≥ximo" in button.text:
                    next_btn = button

        if next_btn:
            next_btn.click()
            page_count += 1
            time.sleep(0.5)
        else:
            break  # N√£o h√° o bot√£o para a pr√≥xima pa«µina. Chegou na √∫ltima p√°gina da se√ß√£o, fim da extra√ß√£o

    driver.quit()
    log("‚úÖ Extra√ß√£o finalizada.")
    return [items, True]


@task
def upload_to_datalake(dou_infos: list, dataset: str) -> None:
    """Fun√ß√£o para realizar a carga dos dados extra√≠dos no datalake da SMS Rio.

    Args:
        dou_infos (dict): Lista com os dicion√°rios contendo os dados extra√≠dos de cada ato do DOU.
        dataset (str): Dataset do BigQuery onde os dados ser√£o carregados.
        successful (bool): Vari√°vel que indica que a extra√ß√£o foi bem sucedida.
        environment (str):
    """

    if dou_infos:
        extracted_at = datetime.now(pytz.timezone("America/Sao_Paulo")).strftime(
            "%Y-%m-%d %H:%M:%S"
        )

        rows = len(dou_infos)
        df = pd.DataFrame(dou_infos)
        df["extracted_at"] = extracted_at
        log(f"Realizando upload de {rows} registros no datalake em {dataset}...")

        upload_df_to_datalake.run(
            df=df,
            dataset_id=dataset,
            table_id="diarios_uniao",
            partition_column="extracted_at",
            if_exists="append",
            if_storage_data_exists="append",
            source_format="csv",
            csv_delimiter=",",
        )
        log("‚úÖ Carregamento no datalake finalizado.")
    else:
        log("‚ùå N√£o h√° dados para carregar.")


@task
def report_extraction_status(status: bool, date: str, dou_section: int, environment: str = "dev"):

    date = parse_date_or_today(date).strftime("%Y-%m-%d")

    success = "true" if status else "false"
    current_datetime = datetime.now(tz=pytz.timezone("America/Sao_Paulo")).replace(tzinfo=None)
    tipo_diario = "dou-sec" + str(dou_section)

    if environment is None:
        environment = "dev"

    PROJECT = constants.GOOGLE_CLOUD_PROJECT.value[environment]
    DATASET = "projeto_cdi"
    TABLE = "extracao_status"
    FULL_TABLE_NAME = f"`{PROJECT}.{DATASET}.{TABLE}`"

    log(f"Inserting into {FULL_TABLE_NAME} status of success={success} for date='{date}'...")

    client = bigquery.Client()
    query_job = client.query(
        f"""
        INSERT INTO {FULL_TABLE_NAME} (
            data_publicacao, tipo_diario, extracao_sucesso, _updated_at
        )
        VALUES (
            '{date}', '{tipo_diario}', {success}, '{current_datetime}'
        )
    """
    )
    query_job.result()
    log("‚úÖ Status da extra√ß√£o reportado!")
